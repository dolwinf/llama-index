{"docstore/metadata": {"40c2caef-1535-4a39-810d-94a945871640": {"doc_hash": "91a61607ccf634f14d4ef49674381223888e497010bd2ff6e9465ab0eea8b29f"}, "553a1392-d74f-4537-8192-852d199d16a4": {"doc_hash": "17815265147a181bab07f5b3e00496a8dabc3c176f6c9a414e317137be55ad21"}, "027220fe-7136-4017-96d1-3e4d5d4f6b3b": {"doc_hash": "737ee4119dade4a16425884de30b33a59736e02b38f799044e4e7ae4790ef101"}, "9acc2326-074f-4dfb-99b6-46b618463bf1": {"doc_hash": "09eba448d83a2bc7fa96d5826d3a0b9a3106ec50617933718bd514d862aa49cf", "ref_doc_id": "40c2caef-1535-4a39-810d-94a945871640"}, "2f76ea2d-0beb-4b0a-9a66-d3d3028a6607": {"doc_hash": "e75e3479b463e303a705a7fd33fdb8698e702aed289f142f4b8adcfd8854908f", "ref_doc_id": "553a1392-d74f-4537-8192-852d199d16a4"}, "74ac4130-e9aa-40b7-9fa5-99c1c4d36792": {"doc_hash": "90e96e06a071ddeab2d4186dac482f105fdfad992a79122ec002fa7bca29536d", "ref_doc_id": "027220fe-7136-4017-96d1-3e4d5d4f6b3b"}}, "docstore/data": {"9acc2326-074f-4dfb-99b6-46b618463bf1": {"__data__": {"id_": "9acc2326-074f-4dfb-99b6-46b618463bf1", "embedding": null, "metadata": {"page_label": "1", "file_name": "llama.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40c2caef-1535-4a39-810d-94a945871640", "node_type": null, "metadata": {"page_label": "1", "file_name": "llama.pdf"}, "hash": "91a61607ccf634f14d4ef49674381223888e497010bd2ff6e9465ab0eea8b29f"}}, "hash": "09eba448d83a2bc7fa96d5826d3a0b9a3106ec50617933718bd514d862aa49cf", "text": "LlamaIndex helps you build LLM -powered applications (e.g. Q&A, chatbot, and \nagents) over custom data.  \nIn this high -level concepts guide, you will learn:  \n\u2022 the retrieval augmented generation (RAG) paradigm for combining LLM \nwith custom data,  \n\u2022 key concepts and modules in LlamaIndex for composing your own RAG \npipeline.  \nRetrieval Augmented Generation \n(RAG)  \nRetrieval augmented generation (RAG) is a paradigm for augmenting LLM with \ncustom data. It generally consists of two stages:  \n1. indexing  stage : preparing a knowledge base, and  \n2. querying  stage : retrieving relevant context from the knowledge to \nassist the LLM in responding to a question  \n \nLlamaIndex provides the essential toolkit for making both steps super easy. \nLet\u2019s explore each stage in detail.  \nIndexing Stage", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2f76ea2d-0beb-4b0a-9a66-d3d3028a6607": {"__data__": {"id_": "2f76ea2d-0beb-4b0a-9a66-d3d3028a6607", "embedding": null, "metadata": {"page_label": "2", "file_name": "llama.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "553a1392-d74f-4537-8192-852d199d16a4", "node_type": null, "metadata": {"page_label": "2", "file_name": "llama.pdf"}, "hash": "17815265147a181bab07f5b3e00496a8dabc3c176f6c9a414e317137be55ad21"}}, "hash": "e75e3479b463e303a705a7fd33fdb8698e702aed289f142f4b8adcfd8854908f", "text": "LlamaIndex help you prepare the knowledge base with a suite of data \nconnectors and \nindexes.  \n \nData  Connectors : A data connector (i.e.  Reader ) ingest data from different data \nsources and data formats into a simple  Document  representation (text and \nsimple metadata).  \nDocuments  / Nodes : A Document  is a generic container around any data source \n- for instance, a PDF, an API output, or retrieved data from a database. \nA Node is the atomic unit of data in LlamaIndex and represents a \u201cchunk\u201d of a \nsource  Document . It\u2019s a rich representation that includes metadata and \nrelationships (to other nodes) to enable accurate and expressive retrieval \noperations.  \nData  Indexes : Once you\u2019ve ingested your data, LlamaIndex will help you index \nthe data into a format that\u2019s easy to retrieve. Under the hood, LlamaIndex \nparses the raw documents into intermediate representations, calculates vector \nembeddings, and infers metadata. The m ost commonly used index is \nthe VectorStoreIndex  \nQuerying Stage  \nIn the querying stage, the RAG pipeline retrieves the most relevant context \ngiven a user query, and pass that to the LLM (along with the query) to \nsynthesize a response. This gives the LLM up -to-date knowledge that is not in \nits original training data, (al so reducing hallucination). The key challenge in the \nquerying stage is retrieval, orchestration, and reasoning over (potentially \nmany) knowledge bases.  \nLlamaIndex provides composable modules that help you build and integrate \nRAG pipelines for Q&A (query engine), chatbot (chat engine), or as part of an \nagent. These building blocks can be customized to reflect ranking preferences, \nas well as composed to rea son over multiple knowledge bases in a structured \nway.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "74ac4130-e9aa-40b7-9fa5-99c1c4d36792": {"__data__": {"id_": "74ac4130-e9aa-40b7-9fa5-99c1c4d36792", "embedding": null, "metadata": {"page_label": "3", "file_name": "llama.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "027220fe-7136-4017-96d1-3e4d5d4f6b3b", "node_type": null, "metadata": {"page_label": "3", "file_name": "llama.pdf"}, "hash": "737ee4119dade4a16425884de30b33a59736e02b38f799044e4e7ae4790ef101"}}, "hash": "90e96e06a071ddeab2d4186dac482f105fdfad992a79122ec002fa7bca29536d", "text": "Building Blocks  \nRetrievers : A retriever defines how to efficiently retrieve relevant context from \na knowledge base (i.e. index) when given a query. The specific retrieval logic \ndiffers for different indices, the most popular being dense retrieval against a \nvector index.  \nNode  Postprocessors : A node postprocessor takes in a set of nodes, then \napply transformation, filtering, or re -ranking logic to them.  \nResponse  Synthesizers : A response synthesizer generates a response from an \nLLM, using a user query and a given set of retrieved text chunks.  \nPipelines  \nQuery  Engines : A query engine is an end -to-end pipeline that allow you to \nask question over your data. It takes in a natural language query, and returns a \nresponse, along with reference context retrieved and passed to the LLM.  \nChat  Engines : A chat engine is an end -to-end pipeline for having a \nconversation with your data (multiple back -and-forth instead of a single \nquestion & answer).  \nAgents : An agent is an automated decision maker (powered by an LLM) that \ninteracts with the world via a set of tools. Agent may be used in the same \nfashion as query engines or chat engines. The main distinction is that an agent \ndynamically decides the best seque nce of actions, instead of following a \npredetermined logic. This gives it additional flexibility to tackle more complex \ntasks.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"40c2caef-1535-4a39-810d-94a945871640": {"node_ids": ["9acc2326-074f-4dfb-99b6-46b618463bf1"], "metadata": {"page_label": "1", "file_name": "llama.pdf"}}, "553a1392-d74f-4537-8192-852d199d16a4": {"node_ids": ["2f76ea2d-0beb-4b0a-9a66-d3d3028a6607"], "metadata": {"page_label": "2", "file_name": "llama.pdf"}}, "027220fe-7136-4017-96d1-3e4d5d4f6b3b": {"node_ids": ["74ac4130-e9aa-40b7-9fa5-99c1c4d36792"], "metadata": {"page_label": "3", "file_name": "llama.pdf"}}}}